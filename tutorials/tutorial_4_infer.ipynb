{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Posterior\n",
    "\n",
    "Inference of the DDM parameters $\\theta$ is performed by passing the trained MNLE to a posterior function. DDM-STRIDE extends the MCMC Posterior function provided by the [sbi package](https://pypi.org/project/sbi/) by allowing inference over parameters given not only the observations $x$ but also experimental conditions $\\pi$. [Markov chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) (MCMC) methods allow to draw correlated samples $\\Theta$ with a probability that depends on the synthetic likelihood $q(x | \\theta, \\pi)$ provided by the MNLE and the prior $p(\\theta)$.\n",
    "Since likelihood and prior are proportional to the posterior, i.e. $p(\\theta | x, \\pi) \\propto q(x | \\theta, \\pi) \\cdot p(\\theta)$, the samples over time approximate the posterior distribution.\n",
    "\n",
    "Open your file in *config/task*. `posterior_params` to control how the posterior samples are drawn. You can find the parameters and their description in the [sbi package](https://github.com/mackelab/sbi/blob/7799de5e4bc676ef4a9db304b225503126735f2c/sbi/inference/posteriors/mcmc_posterior.py#L57). \n",
    "\n",
    "Available MCMC methods include Slice Sampling, Hamiltonian Monte Carlo and the No-U-Turn Sampler. `num_chains` determines how many independent MCMC chains will be run. Increasing the number of chains might yield more precise results especially in case of multimodal distributions. At the start of each chain, a number of `warmup_steps` is performed in order to move towards regions with higher probability before starting to keep the samples. Strongly autocorrelated MCMC chains might lead to many similar samples, hence causing a worse coverage of the posterior. The `thin` parameter allows to reduce the autocorrelation by only accepting every nth sample of a chain. When using `slice_np_vectorized`, the thinning factor is applied to the warmup steps, i.e. with 20 warmup steps and a thinning factor of 5 each chain will perform 100 warmup steps.  \n",
    "As an `init_strategy` for each chain, initial locations can either be drawn from the proposal distribution or selected via Sequential-Importance-Resampling. SIR selects the initial locations with highest probability from a number of initial candidates determined by `init_strategy_num_candidates`. \n",
    "\n",
    "Setting `num_workers` > 1 allows to parallelize and speed up sampling. Since every worker takes up some working memory, you might want to keep an eye on memory utilization.\n",
    "\n",
    "The plots created by the posterior predictive check during the fast diagnosis step in tutorial 5 give an impression of how well the posterior performs. Incrementing `thin`, `init_strategy_num_candidates` and `warmup_steps` or changing the `init_strategy` might increase the run time, but improve the posterior results. For a start, you might want to keep the default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the maximum a posteriori (MAP) parameters \n",
    "\n",
    "The MAP represents the point estimate $\\hat{\\theta}$ with the highest posterior probability. `num_init_samples` initial samples are drawn from the `init_method` that can be chosen as `proposal` or `posterior`. The `num_to_optimize` locations with the highest log probability are selected as initial points and are optimized for `num_iter` optimization steps via gradient ascent.  \n",
    "The parameters of the map can be set in the *config/task* file. If the map values found during the Evaluate stage do not correspond to the maximum of the posterior, try increasing the parameters. You can also use the code cells below to play around with the map. Further information about map can be found in the [sbi package](https://github.com/mackelab/sbi/blob/7799de5e4bc676ef4a9db304b225503126735f2c/sbi/inference/posteriors/mcmc_posterior.py#L503)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access the posterior\n",
    "\n",
    "The subsequent cells allow to access and play around with the posterior. `ddm_stride/sbi_extensions/mcmc.py` provides functions for sampling from the posterior or computing the log potential function $\\log q(x| \\theta, \\pi) \\cdot p(\\theta)$.  \n",
    "You can change the posterior and map parameters via the config file or pass the parameters as arguments to the functions called below. Hover over the functions to get a list of available parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import functions and configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "import torch\n",
    "from ddm_stride.pipeline.evaluate import load_experimental_data\n",
    "from ddm_stride.pipeline.infer import build_posterior, build_prior\n",
    "from ddm_stride.utils.data_names import *\n",
    "from sbi.analysis import ActiveSubspace\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with hydra.initialize(config_path='../config'):\n",
    "    cfg = hydra.compose(config_name='config')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>monkey</th>\n",
       "      <th>rt</th>\n",
       "      <th>coh</th>\n",
       "      <th>correct</th>\n",
       "      <th>choice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.512</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.128</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.512</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6144</th>\n",
       "      <td>2</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.032</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6145</th>\n",
       "      <td>2</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6146</th>\n",
       "      <td>2</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.512</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6147</th>\n",
       "      <td>2</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.128</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6148</th>\n",
       "      <td>2</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6149 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      monkey     rt    coh  correct  choice\n",
       "0          1  0.355  0.512      1.0     0.0\n",
       "1          1  0.359  0.256      1.0     1.0\n",
       "2          1  0.525  0.128      1.0     1.0\n",
       "3          1  0.332  0.512      1.0     1.0\n",
       "4          1  0.302  0.032      0.0     0.0\n",
       "...      ...    ...    ...      ...     ...\n",
       "6144       2  0.627  0.032      1.0     1.0\n",
       "6145       2  0.581  0.256      1.0     1.0\n",
       "6146       2  0.293  0.512      1.0     1.0\n",
       "6147       2  0.373  0.128      1.0     0.0\n",
       "6148       2  0.685  0.000      0.0     1.0\n",
       "\n",
       "[6149 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experimental_data = load_experimental_data(cfg)\n",
    "experimental_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_cond contains the experimental conditions, if available\n",
    "if len(get_experimental_condition_names(cfg)) > 0:\n",
    "    exp_cond = torch.Tensor(experimental_data[:, get_experimental_condition_names(cfg)].values)\n",
    "else: \n",
    "    exp_cond = None\n",
    "\n",
    "# x contains the observations\n",
    "x = torch.Tensor(experimental_data.loc[:, get_observation_names(cfg)].values)\n",
    "\n",
    "# Build the posterior object. You can pass default observations and experimental conditions.\n",
    "# As shown below, x and exp_cond can also be set when sampling or computing probabilities.\n",
    "posterior = build_posterior(cfg, x, exp_cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning bracket width...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:16<00:00,  1.52s/it]s]\n",
      "Generating samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [02:41<00:00,  1.61s/it]\n",
      "Generating samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.57s/it]\n",
      "Running 1 MCMC chains in 1 batches.: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:12<00:00, 252.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: tensor([[-0.8032,  1.7580,  0.6998],\n",
      "        [-0.7950,  1.7599,  0.7000],\n",
      "        [-0.8324,  1.7744,  0.6999],\n",
      "        [-0.7936,  1.7630,  0.6998],\n",
      "        [-0.8173,  1.7626,  0.6996],\n",
      "        [-0.8201,  1.7639,  0.7000],\n",
      "        [-0.7998,  1.7473,  0.7000],\n",
      "        [-0.8237,  1.7621,  0.6999],\n",
      "        [-0.8218,  1.7607,  0.6998],\n",
      "        [-0.8209,  1.7577,  0.6997]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# If you want to draw many samples, you might want to increase the number of chains and workers.\n",
    "samples = posterior.sample((10,), x, exp_cond, num_chains=1, num_workers=1)\n",
    "print(f\"samples: {samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the probability of a parameter $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: tensor([[1.1995, 1.2180, 0.4649],\n",
      "        [0.5047, 1.3627, 0.6425]])\n",
      "log_prob: tensor([[-10481.7188,  -9959.1328]]), \n",
      "potential: tensor([[-10481.7188,  -9959.1328]])\n"
     ]
    }
   ],
   "source": [
    "# Sample two parameters from the prior\n",
    "theta = build_prior(cfg, device='cpu').sample((2,))\n",
    "print(f\"theta: {theta}\")\n",
    "\n",
    "# Compute the log probability for each parameter\n",
    "log_prob = posterior.log_prob(theta, x, exp_cond)\n",
    "potential = posterior.potential(theta, x, exp_cond)\n",
    "\n",
    "print(f\"log_prob: {log_prob}, \\npotential: {potential}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the maximum a posteriori estimate. Map parameters are defined [here](https://github.com/mackelab/sbi/blob/7799de5e4bc676ef4a9db304b225503126735f2c/sbi/inference/posteriors/mcmc_posterior.py#L503)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = posterior.map() \n",
    "print(f\"map: \", map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct a sensitivity analysis. For more information see [sbi tutorial 9](https://github.com/mackelab/sbi/blob/main/tutorials/09_sensitivity_analysis.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = ActiveSubspace(posterior)\n",
    "e_vals, e_vecs = sensitivity.find_directions(posterior_log_prob_as_property=True, num_monte_carlo_samples=100)\n",
    "\n",
    "print(f\"eigenvalues: {e_vals}\")\n",
    "print(f\"eigenvectors: {e_vecs}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cff03ddc82668ca004d587eb356366ca0428f63114de65eccbc236aab7e2a35b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ddm_stride_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

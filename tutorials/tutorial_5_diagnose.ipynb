{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: basic ddm plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing the model\n",
    "\n",
    "The diagnosis step checks if the MNLE has learned the likelihood *P(observations | parameters, experimental conditions)* of data simulated by the DDM specified in `config/ddm_model`. If the likelihood is well approximated, the MNLE emulates the behaviour of the simulator.  \n",
    "Since diagnosis can take quite some time, a fast diagnosis and a slower diagnosis can be run separately. The fast diagnosis allows to observe the behaviour of the MNLE in order to decide if the training should be run again, possibly while increasing `n_wandb_sweeps` in `config/algorithm` or increasing the number of training data simulations in `config/task`. If the results of the fast diagnosis look satisfactory, the slow diagnosis determines if the posterior distribution *P(parameters | observations, experimental conditions)* is well defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fast diagnosis\n",
    "\n",
    "The fast diagnosis utilizes the test data containing multiple i.i.d. observations per combination of inputs (i.e. parameters and experimental conditions). The number of plots that will be created per diagnosis test equals `sim_iid_test_data_params.num_params`.  \n",
    "\n",
    "**Compare observations**\n",
    "\n",
    "This test compares observations simulated by the DDM simulator to observations sampled from the MNLE using the same input parameters and experimental conditions. A well trained MNLE should return observations similar to the simulated observations.  \n",
    "The following figure shows one example plot. The caption of the plot describes the input used for simulating and sampling the observations. The pink histogram shows simulated observations while the blue histogram corresponds to observations sampled from the MNLE.  \n",
    "You will find the compare observation plots at `diagnose/compare_observations.png` in your result folder.\n",
    "\n",
    "<img src=\"tutorial_images/compare_observations.PNG\">\n",
    "\n",
    "\n",
    "**[Posterior Predictive Check](https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssa.12378)**\n",
    "\n",
    "The posterior predictive check allows you to investigate the posterior computed by the MNLE.  \n",
    "Take a look at the example plot below. On the left you can see a blue histogram describing the posterior approximation *P(parameters | observations, experimental conditions)* of each parameter given the simulated *observations* plotted on the right. The pink lines describe the ground truth parameters used for simulating the observations. If the posterior is well approximated, the pink lines should lie somewhere within the blue histogram, as shown below. The grey outline shows an approximation of the prior. If a parameter has been learned correctly, its posterior shape should usually differ from its prior shape, especially if the prior is uniform.  \n",
    "The *simulated samples* histogram on the right shows observations simulated using the posterior samples. The *simulated samples* are supposed to look similar to the initially sampled *observations*.  \n",
    "\n",
    "For more information about the posterior predictive check read [sbi tutorial 12](https://github.com/mackelab/sbi/blob/main/tutorials/12_diagnostics_posterior_predictive_check.ipynb).\n",
    "\n",
    "<img src=\"tutorial_images/posterior_predictive.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slow diagnosis\n",
    "\n",
    "As mentioned before, the slow diagnosis checks if the posterior is well defined. The top left plot in the subsequent image shows you an example ground truth posterior in green. Posterior approximations based on the trained MNLE might be shifted to the left or right, this is called positive bias or negative bias. Moreover, their standard deviation might be too large (overdispersion) or too narrow (underdispersion).\n",
    "\n",
    "**[Simulation-Based Calibration (SBC)](https://arxiv.org/abs/1804.06788)**\n",
    "\n",
    "SBC checks if a bias and/or over-/underdispersion has been found in the posterior approximation. The bottom left image visualizes the test results. A distribution is assumed to be well defined if its line lies within the grey area that represents the 99% confidence region. You can see that this applies to the ground truth plotted in green. The remaining lines serve as a reference for the discernible pattern that each of the poorly defined posteriors will produce.  \n",
    "\n",
    "The plot on the right shows an example of the image the pipeline will create and save in `diagnose/sbc.png` in your result folder. You can observe one line for each parameter. The blue line describing the posterior distribution of the drift parameter is partly located outside the grey area. When comparing the pattern to bottom left plot, it seems that the drift posterior has a slight positive bias. \n",
    "\n",
    "Read [sbi tutorial 13](https://github.com/mackelab/sbi/blob/main/tutorials/13_diagnostics_simulation_based_calibration.ipynb) for more information about SBC.\n",
    "\n",
    "<img src=\"tutorial_images/sbc.PNG\">\n",
    "\n",
    "\n",
    "**[Local coverage test](https://proceedings.mlr.press/v161/zhao21b.html)**\n",
    "\n",
    "The local coverage test is similar to SBC, but tests the posterior distribution locally instead of globally. \n",
    "\n",
    "You can get an overview of the local coverage result by opening `diagnose/global_pvalues.json` that will be created in your results folder. You can see a global p-value for each parameter. A p-value < 0.05 tells you that the posterior is not well defined con respect to this parameter.\n",
    "\n",
    "<img src=\"tutorial_images/p_values.PNG\">\n",
    "\n",
    "The plots saved in `diagnose/local_coverage_check.png` will allow you to get a closer look at the local coverage performance. The subsequent example image visualizes local coverage tests on the posterior *P(parameter | observations, experimental conditions)* specified in the subplot title. The *observations* and *experimental conditions* used for each test are disclosed in the title of each plot.  \n",
    "`diagnose/local_coverage_check.png` only contains plots showing local coverage tests that indicated a bias and/or over-/underdispersion. In the example below a negative bias pattern is identifiable. If all local coverage tests of a parameter indicated the posterior being well defined, no plots are drawn for this parameter. You can choose the maximum number of plots drawn per parameter by adapting `n_local_coverage_plots` in `config/tasks`.\n",
    "\n",
    "TODO: Read [sbi tutorial 14]() for more information about the local coverage test.\n",
    "\n",
    "<img src=\"tutorial_images/local_coverage.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run fast diagnosis step\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../results/${result_folder}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no viable alternative at input '[{'name''\n",
      "See https://hydra.cc/docs/next/advanced/override_grammar/basic for details\n",
      "\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
     ]
    }
   ],
   "source": [
    "%run ../ddm_stride/run.py hydra.run.dir={dir} run_diagnose_fast=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run slow diagnosis step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no viable alternative at input '[{'name''\n",
      "See https://hydra.cc/docs/next/advanced/override_grammar/basic for details\n",
      "\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
     ]
    }
   ],
   "source": [
    "%run ../ddm_stride/run.py hydra.run.dir={dir} run_diagnose_slow=True"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cff03ddc82668ca004d587eb356366ca0428f63114de65eccbc236aab7e2a35b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ddm_stride_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
